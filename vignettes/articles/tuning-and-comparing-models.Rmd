---
title: "Tuning and Comparing Models"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tuning-and-comparing-models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center"
)
library(klaR)
library(mda)
library(rpart)
library(earth)
library(workflowsets)
library(tidymodels)
library(discrim)
theme_set(theme_bw() + theme(legend.position = "top"))
```

Workflow sets are collections of tidymodels workflow objects that are created as a set. A workflow object is a combination of a preprocessor (e.g. a formula or recipe) and a `parsnip` model specification. 

For some problems, users might want to try different combinations of preprocessing options, models, and/or predictor sets. In stead of creating a large number of individual objects, a cohort of workflows can be created simultaneously. 

In this example we'll use a small, two-dimensional data set for illustrating classification models. The data are in the `modeldata` package: 

```{r parabolic}
library(tidymodels)

data(parabolic)
str(parabolic)
```

Let's hold back 25% of the data for a test set: 

```{r 2d-splits}
set.seed(1)
split <- initial_split(parabolic)

train_set <- training(split)
test_set <- testing(split)
```

Visually, we can see that the predictors a mildly correlated and some time of nonlinear class boundary is probably needed. 

```{r 2d-plot, fig.width=5, fig.height=5.1}
ggplot(train_set, aes(x = X1, y = X2, col = class)) + 
  geom_point(alpha = 0.5) + 
  coord_fixed(ratio = 1) + 
  scale_color_brewer(palette = "Dark2")
```
We'll fit two types of discriminant analysis (DA) models (regularized DA and flexible DA using multivariate adaptive regression splines (MARS)) as well as a simple classification tree. Let's create those `parsnip` model objects: 

```{r models}
library(discrim)

mars_disc_spec <- 
  discrim_flexible(prod_degree = tune()) %>% 
  set_engine("earth")

reg_disc_sepc <- 
  discrim_regularized(frac_common_cov = tune(), frac_identity = tune()) %>% 
  set_engine("klaR")

cart_spec <- 
  decision_tree(cost_complexity = tune(), min_n = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")
```

Next, we'll need a resampling method. Let's use the bootstrap

```{r resamples}
set.seed(2)
train_resamples <- bootstraps(train_set)
```

We have a simple data set so, for a preprocessor, a basic formula will suffice. We could also use a recipe as a preprocessor too. 

The workflow set takes a (named) list of preprocessors and a named list of `parsnip` model specifications and can cross them to make all combinations. For our case, it will just make a set of workflows for our models: 

```{r wflow-set}
all_workflows <- 
  workflow_set(
    preproc = list("formula" = class ~ .),
    models = list(regularized = reg_disc_sepc, mars = mars_disc_spec, cart = cart_spec)
  )
all_workflows
```

Since these models all have tuning parameters, we can apply the `workflow_map()` function to execute grid search for each of these models with a common set of arguments. The default function to apply across the workflows is `tune_grid()` but other `tune_*()` functions can be used by passing the function name as the first argument. 

Let's use the same grid size for each model. For the MARS model, there are only two possible tuning parameter values nut `tune_grid()` is forgiving about our request of 20 parameter values. 

The `verbose` option provides a concise listing for which workflow is being processed:

```{r tuning}
all_workflows <- 
  all_workflows %>% 
  workflow_map(resamples = train_resamples, grid = 20, verbose = TRUE)
all_workflows
```

The `result` column now has the results of each `tune_grid()` call. 

From these results, we can get quick assessments of how well these models classified the data: 

```{r rank_res, fig.width=8, fig.height=5.5, out.width="100%"}
rank_results(all_workflows, rank_metric = "roc_auc")

# or a handy plot: 
autoplot(all_workflows, metric = "roc_auc")
```

It looks like the MARS model did well. We can plot its results and also pull out the tuning object too: 

```{r mars, fig.width=6, fig.height=4.25}
autoplot(all_workflows, metric = "roc_auc", id = "formula_mars")
```

Not much of a difference in performance; it may be prudent to use the additive model (via `prod_degree = 1`).

We can also pull out the results of `tune_grid()` for this model: 

```{r mars-results-print}
mars_results <- 
  all_workflows %>% 
  pull_workflow_set_result("formula_mars")
mars_results
```

Let's get that workflow object and finalize the model: 

```{r final-mars}
mars_workflow <- 
  all_workflows %>% 
  pull_workflow("formula_mars")
mars_workflow

mars_workflow_fit <- 
  mars_workflow %>% 
  finalize_workflow(tibble(prod_degree = 1)) %>% 
  fit(data = train_set)
mars_workflow_fit
```

Let's see how well these data work on the test set:

```{r grid-pred}
# Make a grid to predict the whole space:
grid <-
  crossing(X1 = seq(min(train_set$X1), max(train_set$X1), length.out = 250),
           X2 = seq(min(train_set$X1), max(train_set$X2), length.out = 250))

grid <- 
  grid %>% 
  bind_cols(predict(mars_workflow_fit, grid, type = "prob"))
```

We can produce a contour plot for the class boundary then overlay the data: 

```{r 2d-boundary, fig.width=5, fig.height=5.1, warning=FALSE}
ggplot(grid, aes(x = X1, y = X2)) + 
  geom_contour(aes(z = .pred_Class2), breaks = 0.5, col = "black") + 
  geom_point(data = test_set, aes(col = class), alpha = 0.5) + 
  coord_fixed(ratio = 1)+ 
  scale_color_brewer(palette = "Dark2")
```

The workflow set allows us to screen many models to find one that does very well. This can be combined with parallel processing and, especially, racing methods from the [`finetune`](https://finetune.tidymodels.org/reference/tune_race_anova.html) package to optimize the efficiency. 
